# -*- coding: utf-8 -*-
"""Exploratory data analysis 2.0

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vw5vGxi97Ypgmm8tmbvocLzFC50tJCi8
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#all packages 
import nltk 
import string 
import re
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
nltk.download('punkt')
from bs4 import BeautifulSoup
nltk.download('stopwords')
import string #has the list of all punctuations
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['finalflaredata2.csv']))
df.head()

df.ID.nunique(dropna = True)  #no of unique posts by ID

#we don't want duplicate posts, so delete any repetitive posts
df.drop_duplicates(subset=['ID'],keep='first', inplace=True)

df.shape

#check the distribution of posts based on flair - how equally distributed is our dataset
F_count = pd.Series(df['Flair'].value_counts())

F_count.plot.bar()
plt.ylabel('No of posts in flair')
plt.title('Flair distribution across dataset') #We managed to get approximately 200-250 posts per flair, pretty balanced

x = df[df['Body'].isnull()] #905 posts don't have data in body - either drop the column or try considering it 
print(x.shape[0]/2219 * 100)

df['Body'].fillna("") #fill the posts where body is empty with ""

"""Exploring the text in Title - Top words? No of unique words? Should we remove stop words and punctuation?"""

titles = df.Title.str.cat(sep=' ') #convert to string
tokens = word_tokenize(titles)
vocab = set(tokens)
print("No of unique words is " + str(len(vocab))+ " in the Title")
frequency_dist = nltk.FreqDist(tokens)
print(frequency_dist)
#plot the distribution of top 20 words in vocab
frequency_dist.plot(20,cumulative=False)

#Top 20 words after removing stop words
stop_w = stopwords.words('english')
top_w = [word for word in tokens if word not in stop_w]
top_w[0:30] #the top 30 words after removing stop words - we can tell it's better, but still has punctuation

stop_w = stopwords.words('english')

"""Cleaning the data 
1. Remove punctuation  (title, comments, body)
2. Remove stop words    (title, comments, body)
3. Lowercase the text    (title, comments, body)
4. Clean the URLs         (title, comments, body)
"""

columns = ['Title','Body','Comments']

def clean(column):
  #remove stop words
  df[column] = df[column].apply(str)
  df[column] = df[column].str.lower().str.split()
  df[column]=df[column].apply(lambda x: [item for item in x if item not in stop_w and bad_words])
  #remove punctuation
  df[column]=df[column].apply(lambda x: [item for item in x if item not in string.punctuation])
  #lowercase text - already done in models

for col in columns:
  clean(col)

df

#need a substring list  
sub = ['jpg','png','reddit.com/comments']

#function which cleans the URL and extracts important info 
def clean_url(url):
  #if it is an image or no URL which directs to another site
  if any(x in url for x in sub):
    pre = ""
    return pre
  import urllib.parse
  data=''
  address = url
  parsed = urllib.parse.urlsplit(address)
  pre = parsed.path.replace("-"," ")
  pre = pre.replace("/"," ")
  pre = pre.replace("_"," ")
  pre = ' '.join(word for word in pre.split() if word not in stop_w)
  return pre

#take an example to check
clean_url("https://www.altnews.in/arnab-goswamis-video-statement-alleging-attack-doesnt-predate-the-attack-unlike-claimed-by-congress-members/")

df['CleanedURL'] = df['URL'].apply(lambda x: clean_url(x))

df

#convert to string
for column in columns: 
  df[column] = df[column].apply(str)

df['CleanedURL'].apply(str)
df['CleanedURL'] = df['CleanedURL'].str.lower().str.split()
df['CleanedURL']=df['CleanedURL'].apply(lambda x: [item for item in x if item not in stop_w])

df['CleanedURL']

#save the cleaned data to a CSV file 
from google.colab import files
df.to_csv('finalcleanedflaredata.csv') 
files.download('finalcleanedflaredata.csv')

def clean(sample):
  #tokenize
  words = nltk.word_tokenize(sample)
  new_words = []
  #remove punctuation
  for word in words:
      new_word = re.sub(r'[^\w\s]', '', word)
      if new_word != '' and new_word not in stop_w:#remove stop words
          new_words.append(new_word)
   
  return new_words

clean("Hi I'm be take do this Bhumika!!! &yess lit fam#@! but ho!w or what")

